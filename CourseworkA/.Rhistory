######################## WordCloud
### This not requires in the assignment, but still fun to do
# based on https://youtu.be/JoArGkOpeU0
#corpus_T<-clearTweets(tweets_T, c("trump","amp","realdonaldtrump","trumptrain","donald","trumps","alwaystrump")) #remove also some campain slogans
#wordcloud(corpus_T, max.words=50)
#corpus_C<-clearTweets(tweets_C, c("hillary","amp","clinton","hillarys"))
#wordcloud(corpus_C,  max.words=50)
#corpus_B<-clearTweets(tweets_B, c("bernie", "amp", "sanders","bernies"))
#wordcloud(corpus_B,  max.words=50)
##############################
######################## Sentiment analysis
tweets_T.text <- laply(tweets_T, function(t)t$getText()) #get text out of tweets
tweets_C.text <- laply(tweets_C, function(t)t$getText()) #get text out of tweets
tweets_B.text <- laply(tweets_B, function(t)t$getText()) #get text out of tweets
#taken from https://github.com/mjhea0/twitter-sentiment-analysis
pos <- scan('positive-words.txt', what = 'character', comment.char=';') #read the positive words
neg <- scan('negative-words.txt', what = 'character', comment.char=';') #read the negative words
source("sentiment3.R") #load algoritm
# see sentiment3.R form more information about sentiment analysis. It assigns a intereger score
# by substracitng the number of occurrence of negative words from that of positive words
##analysis_T <- score.sentiment(tweets_T.text, pos, neg)
##analysis_C <- score.sentiment(tweets_C.text, pos, neg)
##analysis_B <- score.sentiment(tweets_B.text, pos, neg)
##sem<-data.frame(analysis_T$score, analysis_C$score, analysis_B$score)
##semFrame <-melt(sem, measured=c(analysis_T.score,analysis_C.score, analysis_B.score ))
##names(semFrame) <- c("Candidate", "score")
##semFrame$Candidate <-factor(semFrame$Candidate, labels=c("Donald Trump", "Hillary Clinton", "Bernie Sanders")) # change the labels for your celibrities
#The data you need for the analyses can be found in semFrame
install.packages("openssl",dependencies =T)
library(openssl_1.2.2)
library(openssl)
library(openssl_1.2.2.tar)
library(openssl_1.2.2.)
library(openssl_)
library(openssl)
install.packages(httr)
install.packages("httr")
install.packages("httr")
library(httr)
knitr::opts_chunk$set(echo = TRUE)
#during writing you could add "eval = FALSE",  kntr will than not run this code chunk (take some time do)
setwd("C:\Users\thoru\Google Drive\Tu Delft\Seminar\Coursework A")
#setwd("~/surfdrive/Teaching/own teaching/IN4125 - Seminar Research Methodology for Data Science/2019/coursework A")
# apple , note use / instead of \, which used by windows
#install.packages("twitteR", dependencies = TRUE)
library(twitteR)
#install.packages("RCurl", dependencies = T)
library(RCurl)
#install.packages("bitops", dependencies = T)
library(bitops)
#install.packages("plyr", dependencies = T)
library(plyr)
#install.packages('stringr', dependencies = T)
library(stringr)
#install.packages("NLP", dependencies = T)
library(NLP)
#install.packages("tm", dependencies = T)
library(tm)
#install.packages("wordcloud", dependencies=T)
#install.packages("RColorBrewer", dependencies=TRUE)
library(RColorBrewer)
library(wordcloud)
#install.packages("reshape", dependencies=T)
library(reshape)
#install.packages("openssl",dependencies =T)
library(openssl)
library(base64enc)
#install.packages("httr")
library(httr)
################### functions
clearTweets <- function(tweets, excl) {
tweets.text <- sapply(tweets, function(t)t$getText()) #get text out of tweets
tweets.text = gsub('[[:cntrl:]]', '', tweets.text)
tweets.text = gsub('\\d+', '', tweets.text)
tweets.text <- str_replace_all(tweets.text,"[^[:graph:]]", " ") #remove graphic
corpus <- Corpus(VectorSource(tweets.text))
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus_clean, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, removeWords, c(excl,"http","https","httpst"))
return(corpus_clean)
}
## capture all the output to a file.
################# Collect from Twitter
# for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers
consumer_key <-'k9vUb4jWT1EoYRoCEsE3s8vnc'
consumer_scret <- 'Q6U0Ek1umFvWWGuOVsBOekG7xM5R1cXYkIMx4q85XtWbWqHPo4'
access_token <- '746835948688805889-q9LEUy2zqL8rigTon9FpqNpH6TkSNvk'
access_scret <- 'yc3IFL7cQsEMNCLhXuyUvaOv5Wiick0S7scXRU5G0ZICg'
source("your_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R
setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app
#setup_twitter_oauth("m0c8uiiQq4RtDjAAZTStL92S","qDsqOGPTyISMCS4hqiP8nQ0cGFXZe7a9ek5QpICDjp7XzxIIai", "746835948688805889-pyCZUMD8Cp7OqgjlSNhiLlB3dxzmcMu","h75BZz6Xih9Uwgh4YQgDEPrkoKBnyCQ82iOCZUrlt61xJ") #connect to  twitter app
##### This example uses the following 3 celebrities: Donald Trump, Hillary Clinton, and Bernie Sanders
##  You should replace this with your own celebrities, at least 3, but more preferred
##  Note that it will take the computer some to collect the tweets
tweets_T <- searchTwitter("#trump", n=1000, lang="en", resultType="recent") #1000 recent tweets about Donald Trump, in English (I think that 1500 tweets is max)
tweets_C <- searchTwitter("#hillary", n=1000, lang="en", resultType="recent") #1000 recent tweets about Hillary Clinton
tweets_B <- searchTwitter("#bernie", n=1000, lang="en", resultType="recent") #1000 recent tweets about Bernie Sanders
######################## WordCloud
### This not requires in the assignment, but still fun to do
# based on https://youtu.be/JoArGkOpeU0
#corpus_T<-clearTweets(tweets_T, c("trump","amp","realdonaldtrump","trumptrain","donald","trumps","alwaystrump")) #remove also some campain slogans
#wordcloud(corpus_T, max.words=50)
#corpus_C<-clearTweets(tweets_C, c("hillary","amp","clinton","hillarys"))
#wordcloud(corpus_C,  max.words=50)
#corpus_B<-clearTweets(tweets_B, c("bernie", "amp", "sanders","bernies"))
#wordcloud(corpus_B,  max.words=50)
##############################
######################## Sentiment analysis
tweets_T.text <- laply(tweets_T, function(t)t$getText()) #get text out of tweets
tweets_C.text <- laply(tweets_C, function(t)t$getText()) #get text out of tweets
tweets_B.text <- laply(tweets_B, function(t)t$getText()) #get text out of tweets
#taken from https://github.com/mjhea0/twitter-sentiment-analysis
pos <- scan('positive-words.txt', what = 'character', comment.char=';') #read the positive words
neg <- scan('negative-words.txt', what = 'character', comment.char=';') #read the negative words
source("sentiment3.R") #load algoritm
# see sentiment3.R form more information about sentiment analysis. It assigns a intereger score
# by substracitng the number of occurrence of negative words from that of positive words
##analysis_T <- score.sentiment(tweets_T.text, pos, neg)
##analysis_C <- score.sentiment(tweets_C.text, pos, neg)
##analysis_B <- score.sentiment(tweets_B.text, pos, neg)
##sem<-data.frame(analysis_T$score, analysis_C$score, analysis_B$score)
##semFrame <-melt(sem, measured=c(analysis_T.score,analysis_C.score, analysis_B.score ))
##names(semFrame) <- c("Candidate", "score")
##semFrame$Candidate <-factor(semFrame$Candidate, labels=c("Donald Trump", "Hillary Clinton", "Bernie Sanders")) # change the labels for your celibrities
#The data you need for the analyses can be found in semFrame
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
# for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers
consumer_key <-'k9vUb4jWT1EoYRoCEsE3s8vnc'
consumer_scret <- 'Q6U0Ek1umFvWWGuOVsBOekG7xM5R1cXYkIMx4q85XtWbWqHPo4'
access_token <- '746835948688805889-q9LEUy2zqL8rigTon9FpqNpH6TkSNvk'
access_scret <- 'yc3IFL7cQsEMNCLhXuyUvaOv5Wiick0S7scXRU5G0ZICg'
source("your_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R
setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app
knitr::opts_chunk$set(echo = TRUE)
d <- read.csv("data.csv")
str(d)
d$Model2<-substr(d$model,1,1)
subsetB1 <-d[which(d$model == 'B1'),]
subsetB2 <-d[which(d$model == 'B2'),]
subsetB3 <-d[which(d$model == 'B3'),]
subsetB<-d[which(d$model == 'B1'|d$model == 'B2'|d$model == 'B3'),]
subsetM<-d[which(d$model == 'MN'|d$model == 'M1'|d$model == 'M2'|d$model == 'M3'|d$model == 'MF'),]
subsetS <-d[which(d$model == 'S'),]
AggregateByModel<-aggregate(list(d$score), by =list(d$TeD, d$Model) , mean)
AggregateByType<-aggregate(list(d$score), by =list(d$TeD, d$Model2) , mean)
B<-AggregateByType[(1:7), 3]
M<-AggregateByType[(8:14), 3]
t.test(B, M, paired = TRUE, alternative = "two.sided")
knitr::opts_chunk$set(echo = TRUE)
#during writing you could add "eval = FALSE",  kntr will than not run this code chunk (take some time do)
#setwd("C:\Users\thoru\Google Drive\Tu Delft\Seminar\Coursework A")
#setwd("~/surfdrive/Teaching/own teaching/IN4125 - Seminar Research Methodology for Data Science/2019/coursework A")
# apple , note use / instead of \, which used by windows
#install.packages("twitteR", dependencies = TRUE)
library(twitteR)
#install.packages("RCurl", dependencies = T)
library(RCurl)
#install.packages("bitops", dependencies = T)
library(bitops)
#install.packages("plyr", dependencies = T)
library(plyr)
#install.packages('stringr', dependencies = T)
library(stringr)
#install.packages("NLP", dependencies = T)
library(NLP)
#install.packages("tm", dependencies = T)
library(tm)
#install.packages("wordcloud", dependencies=T)
#install.packages("RColorBrewer", dependencies=TRUE)
library(RColorBrewer)
library(wordcloud)
#install.packages("reshape", dependencies=T)
library(reshape)
#install.packages("openssl")
library(openssl)#for Twitter
library(base64enc)#for twitter
#install.packages("httr")
library(httr)#for twitter
library(car)#for leveneTest
#install.packages("ez")
library(ez)
#install.packages("gplots")
library(gplots) # to plot the means for the celebrities
library(pander)
################### functions
clearTweets <- function(tweets, excl) {
tweets.text <- sapply(tweets, function(t)t$getText()) #get text out of tweets
tweets.text = gsub('[[:cntrl:]]', '', tweets.text)
tweets.text = gsub('\\d+', '', tweets.text)
tweets.text <- str_replace_all(tweets.text,"[^[:graph:]]", " ") #remove graphic
corpus <- Corpus(VectorSource(tweets.text))
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus_clean, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, removeWords, c(excl,"http","https","httpst"))
return(corpus_clean)
}
## capture all the output to a file.
################# Collect from Twitter
# for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers
consumer_key <-'k9vUb4jWT1EoYRoCEsE3s8vnc'
consumer_scret <- 'Q6U0Ek1umFvWWGuOVsBOekG7xM5R1cXYkIMx4q85XtWbWqHPo4'
access_token <- '746835948688805889-q9LEUy2zqL8rigTon9FpqNpH6TkSNvk'
access_scret <- 'yc3IFL7cQsEMNCLhXuyUvaOv5Wiick0S7scXRU5G0ZICg'
#consumer_key <-'S3HE6pNUsmDP3LxMSDgRw8kx8'
#consumer_scret <- '1aLg11NynVF4q93uaxHuCkd9GpIZxtaePxlvVZoKHrmPjrQIoC '
#access_token <- '1079739876080340992-O9MHCV8yIbP4fpCXnktPDKTerIk4i8'
#access_scret <- 'ZYi6mkuPA2zQDw3jqtK9xKvRWc6Yj7xHmCCwMNLpyI0kY'
source("your_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R
setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app
#setup_twitter_oauth("m0c8uiiQq4RtDjAAZTStL92S","qDsqOGPTyISMCS4hqiP8nQ0cGFXZe7a9ek5QpICDjp7XzxIIai", "746835948688805889-pyCZUMD8Cp7OqgjlSNhiLlB3dxzmcMu","h75BZz6Xih9Uwgh4YQgDEPrkoKBnyCQ82iOCZUrlt61xJ") #connect to  twitter app
##### This example uses the following 3 celebrities: Donald Trump, Hillary Clinton, and Bernie Sanders
##  You should replace this with your own celebrities, at least 3, but more preferred
##  Note that it will take the computer some to collect the tweets
tweets_Beyonce <- searchTwitter("#Beyonce", n=1000, lang="en", resultType="recent") #1000 recent tweets about Beyonce, in English
tweets_Madonna <- searchTwitter("#Madonna", n=1000, lang="en", resultType="recent") #1000 recent tweets about Madonna
tweets_West <- searchTwitter("#West", n=1000, lang="en", resultType="recent") #1000 recent tweets about Kanye West
######################## Sentiment analysis
tweets_Beyonce.text <- laply(tweets_Beyonce, function(t)t$getText()) #get text out of tweets
tweets_Madonna.text <- laply(tweets_Madonna, function(t)t$getText()) #get text out of tweets
tweets_West.text <- laply(tweets_West, function(t)t$getText()) #get text out of tweets
#taken from https://github.com/mjhea0/twitter-sentiment-analysis
pos <- scan('positive-words.txt', what = 'character', comment.char=';') #read the positive words
neg <- scan('negative-words.txt', what = 'character', comment.char=';') #read the negative words
source("sentiment3.R") #load algoritm
# see sentiment3.R form more information about sentiment analysis. It assigns a intereger score
# by substracitng the number of occurrence of negative words from that of positive words
analysis_Beyonce <- score.sentiment(tweets_Beyonce.text, pos, neg)
analysis_Madonna <- score.sentiment(tweets_Madonna.text, pos, neg)
analysis_West <- score.sentiment(tweets_West.text, pos, neg)
sem<-data.frame(analysis_Beyonce$score, analysis_Madonna$score, analysis_West$score)
semFrame <-melt(sem, measured=c(analysis_Beyonce.score,analysis_Madonna.score, analysis_West.score ))
names(semFrame) <- c("Person", "score")
semFrame$Person <-factor(semFrame$Person, labels=c("Beyonce", "Madonna", "West")) # change the labels for your celibrities
subsetBeyonce <-semFrame[which(semFrame$Person == 'Beyonce'),]
subsetMadonna <-semFrame[which(semFrame$Person == 'Madonna'),]
subsetWest <-semFrame[which(semFrame$Person == 'West'),]
#The data you need for the analyses can be found in semFrame
#during writing you could add "eval = FALSE",  kntr will than not run this code chunk (take some time do)
#setwd("C:\Users\thoru\Google Drive\Tu Delft\Seminar\Coursework A")
#setwd("~/surfdrive/Teaching/own teaching/IN4125 - Seminar Research Methodology for Data Science/2019/coursework A")
# apple , note use / instead of \, which used by windows
install.packages("twitteR", dependencies = TRUE)
library(twitteR)
#install.packages("RCurl", dependencies = T)
library(RCurl)
#install.packages("bitops", dependencies = T)
library(bitops)
#install.packages("plyr", dependencies = T)
library(plyr)
#install.packages('stringr', dependencies = T)
library(stringr)
#install.packages("NLP", dependencies = T)
library(NLP)
#install.packages("tm", dependencies = T)
library(tm)
#install.packages("wordcloud", dependencies=T)
#install.packages("RColorBrewer", dependencies=TRUE)
library(RColorBrewer)
library(wordcloud)
#install.packages("reshape", dependencies=T)
library(reshape)
#install.packages("openssl")
library(openssl)#for Twitter
library(base64enc)#for twitter
#install.packages("httr")
library(httr)#for twitter
library(car)#for leveneTest
#install.packages("ez")
library(ez)
#install.packages("gplots")
library(gplots) # to plot the means for the celebrities
library(pander)
################### functions
clearTweets <- function(tweets, excl) {
tweets.text <- sapply(tweets, function(t)t$getText()) #get text out of tweets
tweets.text = gsub('[[:cntrl:]]', '', tweets.text)
tweets.text = gsub('\\d+', '', tweets.text)
tweets.text <- str_replace_all(tweets.text,"[^[:graph:]]", " ") #remove graphic
corpus <- Corpus(VectorSource(tweets.text))
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus_clean, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, removeWords, c(excl,"http","https","httpst"))
return(corpus_clean)
}
## capture all the output to a file.
################# Collect from Twitter
# for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers
consumer_key <-'k9vUb4jWT1EoYRoCEsE3s8vnc'
consumer_scret <- 'Q6U0Ek1umFvWWGuOVsBOekG7xM5R1cXYkIMx4q85XtWbWqHPo4'
access_token <- '746835948688805889-q9LEUy2zqL8rigTon9FpqNpH6TkSNvk'
access_scret <- 'yc3IFL7cQsEMNCLhXuyUvaOv5Wiick0S7scXRU5G0ZICg'
#consumer_key <-'S3HE6pNUsmDP3LxMSDgRw8kx8'
#consumer_scret <- '1aLg11NynVF4q93uaxHuCkd9GpIZxtaePxlvVZoKHrmPjrQIoC '
#access_token <- '1079739876080340992-O9MHCV8yIbP4fpCXnktPDKTerIk4i8'
#access_scret <- 'ZYi6mkuPA2zQDw3jqtK9xKvRWc6Yj7xHmCCwMNLpyI0kY'
source("your_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R
setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app
#setup_twitter_oauth("m0c8uiiQq4RtDjAAZTStL92S","qDsqOGPTyISMCS4hqiP8nQ0cGFXZe7a9ek5QpICDjp7XzxIIai", "746835948688805889-pyCZUMD8Cp7OqgjlSNhiLlB3dxzmcMu","h75BZz6Xih9Uwgh4YQgDEPrkoKBnyCQ82iOCZUrlt61xJ") #connect to  twitter app
##### This example uses the following 3 celebrities: Donald Trump, Hillary Clinton, and Bernie Sanders
##  You should replace this with your own celebrities, at least 3, but more preferred
##  Note that it will take the computer some to collect the tweets
tweets_Beyonce <- searchTwitter("#Beyonce", n=1000, lang="en", resultType="recent") #1000 recent tweets about Beyonce, in English
install.packages("twitteR", dependencies = TRUE)
setwd("~/GitHub/SeminarResearchMethodology/CourseworkA")
knitr::opts_chunk$set(echo = TRUE)
#during writing you could add "eval = FALSE",  kntr will than not run this code chunk (take some time do)
#setwd("C:\Users\thoru\Google Drive\Tu Delft\Seminar\Coursework A")
#setwd("~/surfdrive/Teaching/own teaching/IN4125 - Seminar Research Methodology for Data Science/2019/coursework A")
# apple , note use / instead of \, which used by windows
install.packages("twitteR", dependencies = TRUE)
library(twitteR)
#install.packages("RCurl", dependencies = T)
library(RCurl)
#install.packages("bitops", dependencies = T)
library(bitops)
#install.packages("plyr", dependencies = T)
library(plyr)
#install.packages('stringr', dependencies = T)
library(stringr)
#install.packages("NLP", dependencies = T)
library(NLP)
#install.packages("tm", dependencies = T)
library(tm)
#install.packages("wordcloud", dependencies=T)
#install.packages("RColorBrewer", dependencies=TRUE)
library(RColorBrewer)
library(wordcloud)
#install.packages("reshape", dependencies=T)
library(reshape)
#install.packages("openssl")
library(openssl)#for Twitter
library(base64enc)#for twitter
#install.packages("httr")
library(httr)#for twitter
library(car)#for leveneTest
#install.packages("ez")
library(ez)
#install.packages("gplots")
library(gplots) # to plot the means for the celebrities
library(pander)
################### functions
clearTweets <- function(tweets, excl) {
tweets.text <- sapply(tweets, function(t)t$getText()) #get text out of tweets
tweets.text = gsub('[[:cntrl:]]', '', tweets.text)
tweets.text = gsub('\\d+', '', tweets.text)
tweets.text <- str_replace_all(tweets.text,"[^[:graph:]]", " ") #remove graphic
corpus <- Corpus(VectorSource(tweets.text))
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus_clean, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, removeWords, c(excl,"http","https","httpst"))
return(corpus_clean)
}
## capture all the output to a file.
################# Collect from Twitter
# for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers
consumer_key <-'k9vUb4jWT1EoYRoCEsE3s8vnc'
consumer_scret <- 'Q6U0Ek1umFvWWGuOVsBOekG7xM5R1cXYkIMx4q85XtWbWqHPo4'
access_token <- '746835948688805889-q9LEUy2zqL8rigTon9FpqNpH6TkSNvk'
access_scret <- 'yc3IFL7cQsEMNCLhXuyUvaOv5Wiick0S7scXRU5G0ZICg'
#consumer_key <-'S3HE6pNUsmDP3LxMSDgRw8kx8'
#consumer_scret <- '1aLg11NynVF4q93uaxHuCkd9GpIZxtaePxlvVZoKHrmPjrQIoC '
#access_token <- '1079739876080340992-O9MHCV8yIbP4fpCXnktPDKTerIk4i8'
#access_scret <- 'ZYi6mkuPA2zQDw3jqtK9xKvRWc6Yj7xHmCCwMNLpyI0kY'
source("your_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R
setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app
#setup_twitter_oauth("m0c8uiiQq4RtDjAAZTStL92S","qDsqOGPTyISMCS4hqiP8nQ0cGFXZe7a9ek5QpICDjp7XzxIIai", "746835948688805889-pyCZUMD8Cp7OqgjlSNhiLlB3dxzmcMu","h75BZz6Xih9Uwgh4YQgDEPrkoKBnyCQ82iOCZUrlt61xJ") #connect to  twitter app
##### This example uses the following 3 celebrities: Donald Trump, Hillary Clinton, and Bernie Sanders
##  You should replace this with your own celebrities, at least 3, but more preferred
##  Note that it will take the computer some to collect the tweets
tweets_Beyonce <- searchTwitter("#Beyonce", n=1000, lang="en", resultType="recent") #1000 recent tweets about Beyonce, in English
install.packages("twitteR", dependencies = TRUE)
knitr::opts_chunk$set(echo = TRUE)
#during writing you could add "eval = FALSE",  kntr will than not run this code chunk (take some time do)
#setwd("C:\Users\thoru\Google Drive\Tu Delft\Seminar\Coursework A")
#setwd("~/surfdrive/Teaching/own teaching/IN4125 - Seminar Research Methodology for Data Science/2019/coursework A")
# apple , note use / instead of \, which used by windows
#install.packages("twitteR", dependencies = TRUE)
library(twitteR)
#install.packages("RCurl", dependencies = T)
library(RCurl)
#install.packages("bitops", dependencies = T)
library(bitops)
#install.packages("plyr", dependencies = T)
library(plyr)
#install.packages('stringr', dependencies = T)
library(stringr)
#install.packages("NLP", dependencies = T)
library(NLP)
#install.packages("tm", dependencies = T)
library(tm)
#install.packages("wordcloud", dependencies=T)
#install.packages("RColorBrewer", dependencies=TRUE)
library(RColorBrewer)
library(wordcloud)
#install.packages("reshape", dependencies=T)
library(reshape)
#install.packages("openssl")
library(openssl)#for Twitter
library(base64enc)#for twitter
#install.packages("httr")
library(httr)#for twitter
library(car)#for leveneTest
#install.packages("ez")
library(ez)
#install.packages("gplots")
library(gplots) # to plot the means for the celebrities
library(pander)
################### functions
clearTweets <- function(tweets, excl) {
tweets.text <- sapply(tweets, function(t)t$getText()) #get text out of tweets
tweets.text = gsub('[[:cntrl:]]', '', tweets.text)
tweets.text = gsub('\\d+', '', tweets.text)
tweets.text <- str_replace_all(tweets.text,"[^[:graph:]]", " ") #remove graphic
corpus <- Corpus(VectorSource(tweets.text))
corpus_clean <- tm_map(corpus, removePunctuation)
corpus_clean <- tm_map(corpus_clean, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
corpus_clean <- tm_map(corpus_clean, removeWords, c(excl,"http","https","httpst"))
return(corpus_clean)
}
## capture all the output to a file.
################# Collect from Twitter
# for creating a twitter app (apps.twitter.com) see youtube https://youtu.be/lT4Kosc_ers
consumer_key <-'k9vUb4jWT1EoYRoCEsE3s8vnc'
consumer_scret <- 'Q6U0Ek1umFvWWGuOVsBOekG7xM5R1cXYkIMx4q85XtWbWqHPo4'
access_token <- '746835948688805889-q9LEUy2zqL8rigTon9FpqNpH6TkSNvk'
access_scret <- 'yc3IFL7cQsEMNCLhXuyUvaOv5Wiick0S7scXRU5G0ZICg'
#consumer_key <-'S3HE6pNUsmDP3LxMSDgRw8kx8'
#consumer_scret <- '1aLg11NynVF4q93uaxHuCkd9GpIZxtaePxlvVZoKHrmPjrQIoC '
#access_token <- '1079739876080340992-O9MHCV8yIbP4fpCXnktPDKTerIk4i8'
#access_scret <- 'ZYi6mkuPA2zQDw3jqtK9xKvRWc6Yj7xHmCCwMNLpyI0kY'
source("your_twitter.R") #this file will set my personal variables for my twitter app, adjust the name of this file. use the provide template your_twitter.R
setup_twitter_oauth(consumer_key,consumer_scret, access_token,access_scret) #connect to  twitter app
#setup_twitter_oauth("m0c8uiiQq4RtDjAAZTStL92S","qDsqOGPTyISMCS4hqiP8nQ0cGFXZe7a9ek5QpICDjp7XzxIIai", "746835948688805889-pyCZUMD8Cp7OqgjlSNhiLlB3dxzmcMu","h75BZz6Xih9Uwgh4YQgDEPrkoKBnyCQ82iOCZUrlt61xJ") #connect to  twitter app
##### This example uses the following 3 celebrities: Donald Trump, Hillary Clinton, and Bernie Sanders
##  You should replace this with your own celebrities, at least 3, but more preferred
##  Note that it will take the computer some to collect the tweets
tweets_Beyonce <- searchTwitter("#Beyonce", n=1000, lang="en", resultType="recent") #1000 recent tweets about Beyonce, in English
tweets_Madonna <- searchTwitter("#Madonna", n=1000, lang="en", resultType="recent") #1000 recent tweets about Madonna
tweets_West <- searchTwitter("#West", n=1000, lang="en", resultType="recent") #1000 recent tweets about Kanye West
######################## Sentiment analysis
tweets_Beyonce.text <- laply(tweets_Beyonce, function(t)t$getText()) #get text out of tweets
tweets_Madonna.text <- laply(tweets_Madonna, function(t)t$getText()) #get text out of tweets
tweets_West.text <- laply(tweets_West, function(t)t$getText()) #get text out of tweets
#taken from https://github.com/mjhea0/twitter-sentiment-analysis
pos <- scan('positive-words.txt', what = 'character', comment.char=';') #read the positive words
neg <- scan('negative-words.txt', what = 'character', comment.char=';') #read the negative words
source("sentiment3.R") #load algoritm
# see sentiment3.R form more information about sentiment analysis. It assigns a intereger score
# by substracitng the number of occurrence of negative words from that of positive words
analysis_Beyonce <- score.sentiment(tweets_Beyonce.text, pos, neg)
analysis_Madonna <- score.sentiment(tweets_Madonna.text, pos, neg)
analysis_West <- score.sentiment(tweets_West.text, pos, neg)
sem<-data.frame(analysis_Beyonce$score, analysis_Madonna$score, analysis_West$score)
semFrame <-melt(sem, measured=c(analysis_Beyonce.score,analysis_Madonna.score, analysis_West.score ))
names(semFrame) <- c("Person", "score")
semFrame$Person <-factor(semFrame$Person, labels=c("Beyonce", "Madonna", "West")) # change the labels for your celibrities
subsetBeyonce <-semFrame[which(semFrame$Person == 'Beyonce'),]
subsetMadonna <-semFrame[which(semFrame$Person == 'Madonna'),]
subsetWest <-semFrame[which(semFrame$Person == 'West'),]
#The data you need for the analyses can be found in semFrame
View(subsetMadonna)
#include your code and output in the document
hist(subsetBeyonce$score)
hist(subsetMadonna$score)
hist(subsetWest$score)
meanBeyonce<-mean(subsetBeyonce$score)
meanMadonna<-mean(subsetMadonna$score)
meanWest<-mean(subsetWest$score)
#include your code and output in the document
model0<- lm(score ~ 1, data = semFrame) #model without predictor
model1<- lm(score ~ Person, data = semFrame) #model with predictor
AnovaResults <-anova(model0,model1)
#include your code and output in the document
BonferroniResults <- pairwise.t.test(semFrame$score, semFrame$Person, paired = FALSE, p.adjust.method = "bonferroni")
View(subsetMadonna)
AnovaResults$F[2]
AnovaResults$Pr(>F)[2]`
AnovaResults$Pr(>F)[2]
AnovaResults$`Pr(>F)`[2]
AnovaResults$`Pr(>F)`[2]
`r AnovaResults$`Pr(>F)`[2]`
AnovaResults$"Pr(>F)"[2]
AnovaResults$"Pr(>F)"[2]
View(AnovaResults)
AnovaResults[6][2]
AnovaResults[6],2
AnovaResults[6]
AnovaResults[6](2)
AnovaResults[6][2]
AnovaResults$[6][2]
AnovaResults[2, "Pr(>F)"]
View(AnovaResults)
View(BonferroniResults)
View(BonferroniResults)
BonferroniResults
